{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape CPI search results for GAFAM \n",
    "\n",
    "Total number of search pages per platform\n",
    "* Google = 415 pages\n",
    "* Amazon = 188 pages\n",
    "* Facebook = 206 pages\n",
    "* Apple = 229 pages\n",
    "* Microsoft = 107 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cpi(platform, pages):\n",
    "    links = list()\n",
    "    data = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(1, pages+1): \n",
    "        r = requests.get(\"https://www.competitionpolicyinternational.com/page/\" + str(i) + \"/?s=\" + str(platform))\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        temp = soup.select(\"h3.entry-title.td-module-title\")\n",
    "        links.extend(temp)\n",
    "\n",
    "    print(\"Total no. of links: \" + str(len(links)))\n",
    "\n",
    "    for link in links: \n",
    "        r2 = requests.get(link.a[\"href\"])\n",
    "        link_soup = BeautifulSoup(r2.content)\n",
    "        \n",
    "        try: \n",
    "            data.append({\"date\": link_soup.select(\"time.entry-date.updated.td-module-date\")[0].text,\n",
    "                         \"title\": link_soup.select(\"h1.entry-title\")[0].text,\n",
    "                         \"text\": link_soup.select(\"div.td-post-content\")[0].text,\n",
    "                         \"company\": platform})\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        counter += 1\n",
    "        if (counter) % 100 == 0:\n",
    "            print(\"No. links scraped: \" + str(counter))\n",
    "\n",
    "    raw = pd.DataFrame.from_dict(data)\n",
    "    raw.to_csv('data/raw/' + platform + '_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft\n",
    "parse_cpi(\"Microsoft\", 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 2287\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n",
      "No. links scraped: 1200\n",
      "No. links scraped: 1300\n",
      "No. links scraped: 1400\n",
      "No. links scraped: 1500\n",
      "No. links scraped: 1600\n",
      "No. links scraped: 1700\n",
      "No. links scraped: 1800\n",
      "No. links scraped: 1900\n",
      "No. links scraped: 2000\n",
      "No. links scraped: 2100\n",
      "No. links scraped: 2200\n"
     ]
    }
   ],
   "source": [
    "# Apple \n",
    "parse_cpi(\"Apple\", 229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "parse_cpi(\"Facebook\", 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "parse_cpi(\"Amazon\", 188)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 1145\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n"
     ]
    }
   ],
   "source": [
    "# Google\n",
    "parse_cpi(\"Google\", 415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge raw data for all platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.path.join('data/raw/', \"*.csv\")\n",
    "files = glob.glob(files)\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "df = df.drop(df.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11367"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March 25, 2021</td>\n",
       "      <td>Facebook’s Zuckerberg Proposes Changes To Sect...</td>\n",
       "      <td>This week, Facebook CEO Mark Zuckerberg pro...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March 24, 2021</td>\n",
       "      <td>Antitrust in a Digital World: Does It Work? – ...</td>\n",
       "      <td>Below, we have provided the full transcript ...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March 22, 2021</td>\n",
       "      <td>US House Antitrust Chairman Has New Big Tech B...</td>\n",
       "      <td>Democratic Representative David Cicilline, ...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March 22, 2021</td>\n",
       "      <td>Biden Nominates Antitrust Expert Lina Khan For...</td>\n",
       "      <td>US President Joe Biden intends to nominate ...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>March 19, 2021</td>\n",
       "      <td>Facebook May Face New Antitrust Probe In UK</td>\n",
       "      <td>Britain’s competition regulator is set to b...</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                                              title  \\\n",
       "0  March 25, 2021  Facebook’s Zuckerberg Proposes Changes To Sect...   \n",
       "1  March 24, 2021  Antitrust in a Digital World: Does It Work? – ...   \n",
       "2  March 22, 2021  US House Antitrust Chairman Has New Big Tech B...   \n",
       "3  March 22, 2021  Biden Nominates Antitrust Expert Lina Khan For...   \n",
       "4  March 19, 2021        Facebook May Face New Antitrust Probe In UK   \n",
       "\n",
       "                                                text company  \n",
       "0     This week, Facebook CEO Mark Zuckerberg pro...  Google  \n",
       "1    Below, we have provided the full transcript ...  Google  \n",
       "2     Democratic Representative David Cicilline, ...  Google  \n",
       "3     US President Joe Biden intends to nominate ...  Google  \n",
       "4     Britain’s competition regulator is set to b...  Google  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unrelated paragraphs (e.g. Read More: & Related:)\n",
    "pattern = \"[\\w\\s]+:[\\w\\s]+[^a-zA-Z\\d\\s]+[\\w\\s]+\\\\n\"\n",
    "df['text'] = [re.sub(pattern, \" \", x) for x in df['text']]\n",
    "\n",
    "# Remove white spaces\n",
    "df['text'] = [x.replace(\"\\n\", \"\") for x in df['text']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER to extract countries from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pycountry\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = spacy.load(\"en_core_web_sm\") # smaller pipeline\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = pd.read_csv(\"data/others/country_nationality_list.csv\")\n",
    "legend = legend.set_index('nationality').to_dict()['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract info from string         \n",
    "def extract(ner_function, pipeline, texts):\n",
    "    list = []\n",
    "    for text in texts:\n",
    "        raw = str(ner_function(pipeline, text))\n",
    "        list.append(str(raw))            \n",
    "    return (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner get countries and nationalities\n",
    "def get_countries(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'GPE': # country, city, state\n",
    "            if entity.text in legend.values(): # keep if in country list\n",
    "                return lm.lemmatize(entity.text)\n",
    "            else: \n",
    "                return None            \n",
    "        elif entity.label_ == 'NORP': # nationality \n",
    "            if entity.text in legend.keys(): # match to country\n",
    "                return legend[entity.text]\n",
    "            else:\n",
    "                 return None              \n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check title & text for values\n",
    "def check_both(list1, list2):\n",
    "    list = []\n",
    "    for i, j in zip(list1, list2):\n",
    "        if j != 'None':\n",
    "            list.append(j)\n",
    "        else:\n",
    "            list.append(i)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "texts = df['text']\n",
    "\n",
    "c_title = extract(get_countries, nlp_lg, titles)\n",
    "c_text = extract(get_countries, nlp_lg, texts)\n",
    "\n",
    "countries = check_both(c_title, c_text)\n",
    "\n",
    "# titles that mention cities instead of countries not captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7023"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.count('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER to extract fines from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[^a-zA-Z\\d\\s:]?\\d*\\.?\\d*\\-?\\s?\\w+illion$\"\n",
    "\n",
    "# ner get fines\n",
    "def get_fines(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'MONEY': \n",
    "            match = re.findall(pattern, entity.text)\n",
    "            return match\n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_title = extract(get_fines, nlp_lg, titles)\n",
    "f_text = extract(get_fines, nlp_lg, texts)\n",
    "\n",
    "fines = check_both(f_title, f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11281"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fines.count('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['None', \"['$1 billion']\", \"['$1 million']\", \"['$1.36 billion']\",\n",
       "       \"['$1.7 billion']\", \"['$1.8 billion']\", \"['$100 million']\",\n",
       "       \"['$15 billion']\", \"['$16 billion']\", \"['$16.48 million']\",\n",
       "       \"['$17 million']\", \"['$19 billion']\", \"['$25 billion']\",\n",
       "       \"['$26 billion']\", \"['$3 billion']\", \"['$3.84 billion']\",\n",
       "       \"['$300 billion']\", \"['$324.5 million']\", \"['$37 billion']\",\n",
       "       \"['$375 million']\", \"['$4 billion']\", \"['$4.5 billion']\",\n",
       "       \"['$415 million']\", \"['$49 billion']\", \"['$5 billion']\",\n",
       "       \"['$50 million']\", \"['$600 million']\", \"['$69 Billion']\",\n",
       "       \"['$7.4 billion']\", \"['$774 million']\", \"['£22 billion']\",\n",
       "       \"['£4-trillion']\", \"['€1.49 billion']\", \"['€3 billion']\", '[]'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_fines = pd.DataFrame.from_dict({'country':countries, 'fine':fines})\n",
    "df_clean = pd.concat([df, country_fines], axis=1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unavailable articles\n",
    "unavail = \"THIS ARTICLE IS NOT AVAILABLE FOR IP ADDRESS\"\n",
    "df_clean = df_clean[df_clean[\"text\"].str.contains(unavail) == False]\n",
    "\n",
    "# keep only articles that mention GAFAM\n",
    "platforms = [\"Google\", \"Amazon\", \"Facebook\", \"Apple\", \"Microsoft\"]\n",
    "df_clean = df_clean[df_clean[\"text\"].str.contains('|'.join(platforms))]\n",
    "\n",
    "# format date variable\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "df_clean['year'] = [x.year for x in df_clean['date']]\n",
    "\n",
    "df_clean = df_clean.drop(df_clean.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>fine</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>Facebook’s Zuckerberg Proposes Changes To Sect...</td>\n",
       "      <td>This week, Facebook CEO Mark Zuckerberg pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>US House Antitrust Chairman Has New Big Tech B...</td>\n",
       "      <td>Democratic Representative David Cicilline, ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>Biden Nominates Antitrust Expert Lina Khan For...</td>\n",
       "      <td>US President Joe Biden intends to nominate ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>Facebook May Face New Antitrust Probe In UK</td>\n",
       "      <td>Britain’s competition regulator is set to b...</td>\n",
       "      <td>Google</td>\n",
       "      <td>UK</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>Facebook Agrees To Pay News Corp For Content I...</td>\n",
       "      <td>News Corp has struck a three-year deal to pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Australia</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-25  Facebook’s Zuckerberg Proposes Changes To Sect...   \n",
       "1 2021-03-22  US House Antitrust Chairman Has New Big Tech B...   \n",
       "2 2021-03-22  Biden Nominates Antitrust Expert Lina Khan For...   \n",
       "3 2021-03-19        Facebook May Face New Antitrust Probe In UK   \n",
       "4 2021-03-16  Facebook Agrees To Pay News Corp For Content I...   \n",
       "\n",
       "                                                text company    country  fine  \\\n",
       "0     This week, Facebook CEO Mark Zuckerberg pro...  Google       None  None   \n",
       "1     Democratic Representative David Cicilline, ...  Google       None  None   \n",
       "2     US President Joe Biden intends to nominate ...  Google         US  None   \n",
       "3     Britain’s competition regulator is set to b...  Google         UK  None   \n",
       "4   News Corp has struck a three-year deal to pro...  Google  Australia  None   \n",
       "\n",
       "   year  \n",
       "0  2021  \n",
       "1  2021  \n",
       "2  2021  \n",
       "3  2021  \n",
       "4  2021  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7608"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"data/df_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(df_clean['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [07:03<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(text, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimension of embeddings & cluster similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(text, columns = ['text'])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/tad-project/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>81</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>105</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>76</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>107</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>104</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Size\n",
       "0       -1  2813\n",
       "82      81   793\n",
       "106    105   371\n",
       "77      76   168\n",
       "1        0   110\n",
       "73      72    96\n",
       "99      98    92\n",
       "108    107    83\n",
       "105    104    79\n",
       "28      27    74"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .text\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"text\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deal', 0.009761933632152565),\n",
       " ('billion', 0.007693816306387451),\n",
       " ('acquisition', 0.005651891442568295),\n",
       " ('announced', 0.004868824607617178),\n",
       " ('cloud', 0.004733477069163744),\n",
       " ('buy', 0.004469146807368187),\n",
       " ('approval', 0.00426996103415094),\n",
       " ('walmart', 0.004098650763737676),\n",
       " ('microsoft', 0.003919413346215811),\n",
       " ('activision', 0.0038622241087137646)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[81][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judge', 0.011200788119330654),\n",
       " ('iphone', 0.010738869558496393),\n",
       " ('cote', 0.010507861991163422),\n",
       " ('epic', 0.008570872640576915),\n",
       " ('ebooks', 0.008569484704532525),\n",
       " ('bromwich', 0.006881275695260066),\n",
       " ('store', 0.006876899357315192),\n",
       " ('ruling', 0.006581954820400608),\n",
       " ('developers', 0.006431557401341426),\n",
       " ('app', 0.006175678419772144)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[105][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('australian', 0.02084943203893733),\n",
       " ('accc', 0.019020900445906645),\n",
       " ('australia', 0.017932806241640478),\n",
       " ('code', 0.017043954788835934),\n",
       " ('inquiry', 0.008977780322464416),\n",
       " ('media', 0.00887345312456362),\n",
       " ('sims', 0.008389804393621966),\n",
       " ('publishers', 0.007958226487556248),\n",
       " ('au', 0.00793549621143133),\n",
       " ('mandatory', 0.006872204682367001)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[76][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tad-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cab4409ce5d224ee9075dc374e4bd34bc4b9571e804fbf24e5d7f1c9b19a09d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
