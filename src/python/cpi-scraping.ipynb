{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape CPI search results for GAFAM \n",
    "\n",
    "Total number of search pages per platform\n",
    "* Google = 415 pages\n",
    "* Amazon = 188 pages\n",
    "* Facebook = 206 pages\n",
    "* Apple = 229 pages\n",
    "* Microsoft = 107 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cpi(platform, pages):\n",
    "    links = list()\n",
    "    data = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(1, pages+1): \n",
    "        r = requests.get(\"https://www.competitionpolicyinternational.com/page/\" + str(i) + \"/?s=\" + str(platform))\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        temp = soup.select(\"h3.entry-title.td-module-title\")\n",
    "        links.extend(temp)\n",
    "\n",
    "    print(\"Total no. of links: \" + str(len(links)))\n",
    "\n",
    "    for link in links: \n",
    "        r2 = requests.get(link.a[\"href\"])\n",
    "        link_soup = BeautifulSoup(r2.content)\n",
    "        \n",
    "        try: \n",
    "            data.append({\"date\": link_soup.select(\"time.entry-date.updated.td-module-date\")[0].text,\n",
    "                         \"title\": link_soup.select(\"h1.entry-title\")[0].text,\n",
    "                         \"text\": link_soup.select(\"div.td-post-content\")[0].text,\n",
    "                         \"company_search\": platform})\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        counter += 1\n",
    "        if (counter) % 100 == 0:\n",
    "            print(\"No. links scraped: \" + str(counter))\n",
    "\n",
    "    raw = pd.DataFrame.from_dict(data)\n",
    "    raw.to_csv('data/raw/' + platform + '_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft\n",
    "parse_cpi(\"Microsoft\", 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 2287\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n",
      "No. links scraped: 1200\n",
      "No. links scraped: 1300\n",
      "No. links scraped: 1400\n",
      "No. links scraped: 1500\n",
      "No. links scraped: 1600\n",
      "No. links scraped: 1700\n",
      "No. links scraped: 1800\n",
      "No. links scraped: 1900\n",
      "No. links scraped: 2000\n",
      "No. links scraped: 2100\n",
      "No. links scraped: 2200\n"
     ]
    }
   ],
   "source": [
    "# Apple \n",
    "parse_cpi(\"Apple\", 229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "parse_cpi(\"Facebook\", 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "parse_cpi(\"Amazon\", 188)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 1145\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n"
     ]
    }
   ],
   "source": [
    "# Google\n",
    "parse_cpi(\"Google\", 415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge raw data for all platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.path.join(f\"../../data/raw/\", \"*.csv\")\n",
    "files = glob.glob(files)\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "df = df.drop(df.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>November 14, 2022</td>\n",
       "      <td>Germany Widens Amazon Investigation Under New ...</td>\n",
       "      <td>\\r\\r\\rGermany’s antitrust watchdog said on Mon...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>November 10, 2022</td>\n",
       "      <td>New US Suit Claims Apple, Amazon Colluded To R...</td>\n",
       "      <td>\\r\\r\\rApple and Amazon were accused in an anti...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>October 31, 2022</td>\n",
       "      <td>Italian Court Sends Decision On Amazon Appeal ...</td>\n",
       "      <td>\\r\\r\\rTech giant’s appeal against $1.8 billion...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>October 27, 2022</td>\n",
       "      <td>Amazon Donates To Conservative Nonprofit To Op...</td>\n",
       "      <td>\\r\\r\\rAmazon quietly donated $400,000 to a con...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October 20, 2022</td>\n",
       "      <td>Amazon Faces UK Antitrust Class Action For Alg...</td>\n",
       "      <td>\\r\\r\\rAmazon is facing a $1 billion class acti...</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                              title  \\\n",
       "0  November 14, 2022  Germany Widens Amazon Investigation Under New ...   \n",
       "1  November 10, 2022  New US Suit Claims Apple, Amazon Colluded To R...   \n",
       "2   October 31, 2022  Italian Court Sends Decision On Amazon Appeal ...   \n",
       "3   October 27, 2022  Amazon Donates To Conservative Nonprofit To Op...   \n",
       "4   October 20, 2022  Amazon Faces UK Antitrust Class Action For Alg...   \n",
       "\n",
       "                                                text company  \n",
       "0  \\r\\r\\rGermany’s antitrust watchdog said on Mon...  Amazon  \n",
       "1  \\r\\r\\rApple and Amazon were accused in an anti...  Amazon  \n",
       "2  \\r\\r\\rTech giant’s appeal against $1.8 billion...  Amazon  \n",
       "3  \\r\\r\\rAmazon quietly donated $400,000 to a con...  Amazon  \n",
       "4  \\r\\r\\rAmazon is facing a $1 billion class acti...  Amazon  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unrelated paragraphs (e.g. Read More: & Related:)\n",
    "pattern = \"[\\w\\s]+:[\\w\\s]+[^a-zA-Z\\d\\s]+[\\w\\s]+\\\\n\"\n",
    "df['text'] = [re.sub(pattern, \" \", x) for x in df['text']]\n",
    "\n",
    "# Remove white spaces\n",
    "df['text'] = [x.replace(\"\\n\", \"\") for x in df['text']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER to extract countries from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-lg==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from en-core-web-lg==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.23.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\antitrust\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pycountry\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = spacy.load(\"en_core_web_sm\") # smaller pipeline\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = pd.read_csv(os.path.join(f\"../../data/others/\", \"country_nationality_list.csv\"))\n",
    "legend = legend.set_index('nationality').to_dict()['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract info from string         \n",
    "def extract(ner_function, pipeline, texts):\n",
    "    list = []\n",
    "    for text in texts:\n",
    "        raw = str(ner_function(pipeline, text))\n",
    "        list.append(str(raw))            \n",
    "    return (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner get countries and nationalities\n",
    "def get_countries(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'GPE': # country, city, state\n",
    "            if entity.text in legend.values(): # keep if in country list\n",
    "                return lm.lemmatize(entity.text)\n",
    "            else: \n",
    "                return None            \n",
    "        elif entity.label_ == 'NORP': # nationality \n",
    "            if entity.text in legend.keys(): # match to country\n",
    "                return legend[entity.text]\n",
    "            else:\n",
    "                 return None              \n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check title & text for values\n",
    "def check_both(list1, list2):\n",
    "    list = []\n",
    "    for i, j in zip(list1, list2):\n",
    "        if j != 'None':\n",
    "            list.append(j)\n",
    "        else:\n",
    "            list.append(i)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "texts = df['text']\n",
    "\n",
    "c_title = extract(get_countries, nlp_lg, titles)\n",
    "c_text = extract(get_countries, nlp_lg, texts)\n",
    "\n",
    "countries = check_both(c_title, c_text)\n",
    "\n",
    "# titles that mention cities instead of countries not captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.count('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER to extract fines from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"[^a-zA-Z\\d\\s:]?\\d*\\.?\\d*\\-?\\s?\\w+illion$\"\n",
    "\n",
    "# ner get fines\n",
    "def get_fines(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'MONEY': \n",
    "            match = re.findall(pattern, entity.text)\n",
    "            return match\n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_title = extract(get_fines, nlp_lg, titles)\n",
    "f_text = extract(get_fines, nlp_lg, texts)\n",
    "\n",
    "fines = check_both(f_title, f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11281"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fines.count('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['None', \"['$1 billion']\", \"['$1 million']\", \"['$1.36 billion']\",\n",
       "       \"['$1.7 billion']\", \"['$1.8 billion']\", \"['$100 million']\",\n",
       "       \"['$15 billion']\", \"['$16 billion']\", \"['$16.48 million']\",\n",
       "       \"['$17 million']\", \"['$19 billion']\", \"['$25 billion']\",\n",
       "       \"['$26 billion']\", \"['$3 billion']\", \"['$3.84 billion']\",\n",
       "       \"['$300 billion']\", \"['$324.5 million']\", \"['$37 billion']\",\n",
       "       \"['$375 million']\", \"['$4 billion']\", \"['$4.5 billion']\",\n",
       "       \"['$415 million']\", \"['$49 billion']\", \"['$5 billion']\",\n",
       "       \"['$50 million']\", \"['$600 million']\", \"['$69 Billion']\",\n",
       "       \"['$7.4 billion']\", \"['$774 million']\", \"['£22 billion']\",\n",
       "       \"['£4-trillion']\", \"['€1.49 billion']\", \"['€3 billion']\", '[]'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_fines = pd.DataFrame.from_dict({'country':countries, 'fine':fines})\n",
    "df_clean = pd.concat([df, country_fines], axis=1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unavailable articles\n",
    "unavail = \"THIS ARTICLE IS NOT AVAILABLE FOR IP ADDRESS\"\n",
    "df_clean = df_clean[df_clean['text'].str.contains(unavail) == False]\n",
    "\n",
    "# keep only articles that mention GAFAM\n",
    "platforms = [\"Google\", \"Amazon\", \"Facebook\", \"Apple\", \"Microsoft\"]\n",
    "df_clean = df_clean[df_clean['text'].str.contains('|'.join(platforms))]\n",
    "\n",
    "# format date variable\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "df_clean['year'] = [x.year for x in df_clean['date']]\n",
    "\n",
    "# remove unrelated text \n",
    "string = \"Subscribe to CPI’s free daily newsletter for more headlines and updates on antitrust developments around the world.\"\n",
    "df_clean['text'] = [re.sub(string, \" \", x) for x in df_clean['text']]\n",
    "\n",
    "# drop duplicates, keep first company\n",
    "df_clean = df_clean.drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "# company variable based on title \n",
    "df_clean['company_title'] = [re.findall('|'.join(platforms), x) for x in df_clean['title']]\n",
    "\n",
    "# dummy variables for companies (based on title)\n",
    "dummies = df_clean.title.str.findall('|'.join(platforms)).str[0].str.get_dummies()\n",
    "df_clean = pd.concat([df_clean,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>fine</th>\n",
       "      <th>year</th>\n",
       "      <th>company_title</th>\n",
       "      <th>Amazon</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>Facebook’s Zuckerberg Proposes Changes To Sect...</td>\n",
       "      <td>This week, Facebook CEO Mark Zuckerberg pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>US House Antitrust Chairman Has New Big Tech B...</td>\n",
       "      <td>Democratic Representative David Cicilline, ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>Biden Nominates Antitrust Expert Lina Khan For...</td>\n",
       "      <td>US President Joe Biden intends to nominate ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>Facebook May Face New Antitrust Probe In UK</td>\n",
       "      <td>Britain’s competition regulator is set to b...</td>\n",
       "      <td>Google</td>\n",
       "      <td>UK</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>Facebook Agrees To Pay News Corp For Content I...</td>\n",
       "      <td>News Corp has struck a three-year deal to pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Australia</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-25  Facebook’s Zuckerberg Proposes Changes To Sect...   \n",
       "1 2021-03-22  US House Antitrust Chairman Has New Big Tech B...   \n",
       "2 2021-03-22  Biden Nominates Antitrust Expert Lina Khan For...   \n",
       "3 2021-03-19        Facebook May Face New Antitrust Probe In UK   \n",
       "4 2021-03-16  Facebook Agrees To Pay News Corp For Content I...   \n",
       "\n",
       "                                                text company    country  fine  \\\n",
       "0     This week, Facebook CEO Mark Zuckerberg pro...  Google       None  None   \n",
       "1     Democratic Representative David Cicilline, ...  Google       None  None   \n",
       "2     US President Joe Biden intends to nominate ...  Google         US  None   \n",
       "3     Britain’s competition regulator is set to b...  Google         UK  None   \n",
       "4   News Corp has struck a three-year deal to pro...  Google  Australia  None   \n",
       "\n",
       "   year company_title  Amazon  Apple  Facebook  Google  Microsoft  \n",
       "0  2021    [Facebook]       0      0         1       0          0  \n",
       "1  2021            []       0      0         0       0          0  \n",
       "2  2021            []       0      0         0       0          0  \n",
       "3  2021    [Facebook]       0      0         1       0          0  \n",
       "4  2021    [Facebook]       0      0         1       0          0  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.drop(df_clean.columns[0], axis = 1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4730"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"data/df_clean.csv\")\n",
    "#df_clean = pd.read_csv(\"data/df_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f\"../../data/df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ja'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "detect('今一はお前さん')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check language based on headline\n",
    "for index, row in df1.iterrows():\n",
    "    df1.at[index, 'lang'] = detect(df1.at[index, 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>company_search</th>\n",
       "      <th>country</th>\n",
       "      <th>fine</th>\n",
       "      <th>year</th>\n",
       "      <th>company_title</th>\n",
       "      <th>Amazon</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Google</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>Facebook’s Zuckerberg Proposes Changes To Sect...</td>\n",
       "      <td>This week, Facebook CEO Mark Zuckerberg pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>US House Antitrust Chairman Has New Big Tech B...</td>\n",
       "      <td>Democratic Representative David Cicilline, ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>Biden Nominates Antitrust Expert Lina Khan For...</td>\n",
       "      <td>US President Joe Biden intends to nominate ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>Facebook May Face New Antitrust Probe In UK</td>\n",
       "      <td>Britain’s competition regulator is set to b...</td>\n",
       "      <td>Google</td>\n",
       "      <td>UK</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>Facebook Agrees To Pay News Corp For Content I...</td>\n",
       "      <td>News Corp has struck a three-year deal to pro...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Australia</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>[Facebook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-25  Facebook’s Zuckerberg Proposes Changes To Sect...   \n",
       "1 2021-03-22  US House Antitrust Chairman Has New Big Tech B...   \n",
       "2 2021-03-22  Biden Nominates Antitrust Expert Lina Khan For...   \n",
       "3 2021-03-19        Facebook May Face New Antitrust Probe In UK   \n",
       "4 2021-03-16  Facebook Agrees To Pay News Corp For Content I...   \n",
       "\n",
       "                                                text company_search  \\\n",
       "0     This week, Facebook CEO Mark Zuckerberg pro...         Google   \n",
       "1     Democratic Representative David Cicilline, ...         Google   \n",
       "2     US President Joe Biden intends to nominate ...         Google   \n",
       "3     Britain’s competition regulator is set to b...         Google   \n",
       "4   News Corp has struck a three-year deal to pro...         Google   \n",
       "\n",
       "     country  fine  year company_title  Amazon  Apple  Facebook  Google  \\\n",
       "0       None  None  2021    [Facebook]       0      0         1       0   \n",
       "1       None  None  2021            []       0      0         0       0   \n",
       "2         US  None  2021            []       0      0         0       0   \n",
       "3         UK  None  2021    [Facebook]       0      0         1       0   \n",
       "4  Australia  None  2021    [Facebook]       0      0         1       0   \n",
       "\n",
       "   Microsoft lang  \n",
       "0          0   en  \n",
       "1          0   en  \n",
       "2          0   en  \n",
       "3          0   en  \n",
       "4          0   en  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['af', 'ca', 'cy', 'da', 'de', 'en', 'es', 'et', 'fr', 'hu', 'id',\n",
       "       'it', 'nl', 'no', 'pt', 'ro', 'sk', 'so', 'sv', 'sw', 'tl'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df1.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    4256\n",
       "es     139\n",
       "de      97\n",
       "fr      56\n",
       "no      26\n",
       "it      24\n",
       "da      23\n",
       "ca      21\n",
       "af      14\n",
       "nl      13\n",
       "pt      13\n",
       "tl      12\n",
       "ro       8\n",
       "so       8\n",
       "id       5\n",
       "et       5\n",
       "cy       5\n",
       "sv       2\n",
       "hu       1\n",
       "sw       1\n",
       "sk       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    4256\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_eng = df1[df1['lang']==\"en\"]\n",
    "df_clean_eng.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_eng.to_csv(f\"../../data/df_clean_eng.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(df_clean['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [07:03<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(text, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimension of embeddings & cluster similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(text, columns = ['text'])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/tad-project/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>81</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>105</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>76</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>107</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>104</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Size\n",
       "0       -1  2813\n",
       "82      81   793\n",
       "106    105   371\n",
       "77      76   168\n",
       "1        0   110\n",
       "73      72    96\n",
       "99      98    92\n",
       "108    107    83\n",
       "105    104    79\n",
       "28      27    74"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .text\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"text\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deal', 0.009761933632152565),\n",
       " ('billion', 0.007693816306387451),\n",
       " ('acquisition', 0.005651891442568295),\n",
       " ('announced', 0.004868824607617178),\n",
       " ('cloud', 0.004733477069163744),\n",
       " ('buy', 0.004469146807368187),\n",
       " ('approval', 0.00426996103415094),\n",
       " ('walmart', 0.004098650763737676),\n",
       " ('microsoft', 0.003919413346215811),\n",
       " ('activision', 0.0038622241087137646)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[81][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judge', 0.011200788119330654),\n",
       " ('iphone', 0.010738869558496393),\n",
       " ('cote', 0.010507861991163422),\n",
       " ('epic', 0.008570872640576915),\n",
       " ('ebooks', 0.008569484704532525),\n",
       " ('bromwich', 0.006881275695260066),\n",
       " ('store', 0.006876899357315192),\n",
       " ('ruling', 0.006581954820400608),\n",
       " ('developers', 0.006431557401341426),\n",
       " ('app', 0.006175678419772144)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[105][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('australian', 0.02084943203893733),\n",
       " ('accc', 0.019020900445906645),\n",
       " ('australia', 0.017932806241640478),\n",
       " ('code', 0.017043954788835934),\n",
       " ('inquiry', 0.008977780322464416),\n",
       " ('media', 0.00887345312456362),\n",
       " ('sims', 0.008389804393621966),\n",
       " ('publishers', 0.007958226487556248),\n",
       " ('au', 0.00793549621143133),\n",
       " ('mandatory', 0.006872204682367001)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[76][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('antitrust')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704f21e3caa22f96781619853c195d98e7c4152b70fa439c60c053a2c2e9d574"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
