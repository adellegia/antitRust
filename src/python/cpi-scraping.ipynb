{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape CPI search results for GAFAM \n",
    "\n",
    "Total number of search pages per platform\n",
    "* Google = 415 pages\n",
    "* Amazon = 188 pages\n",
    "* Facebook = 206 pages\n",
    "* Apple = 229 pages\n",
    "* Microsoft = 107 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cpi(platform, pages):\n",
    "    links = list()\n",
    "    data = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(1, pages+1): \n",
    "        r = requests.get(\"https://www.competitionpolicyinternational.com/page/\" + str(i) + \"/?s=\" + str(platform))\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        temp = soup.select(\"h3.entry-title.td-module-title\")\n",
    "        links.extend(temp)\n",
    "\n",
    "    print(\"Total no. of links: \" + str(len(links)))\n",
    "\n",
    "    for link in links: \n",
    "        r2 = requests.get(link.a[\"href\"])\n",
    "        link_soup = BeautifulSoup(r2.content)\n",
    "        \n",
    "        try: \n",
    "            data.append({\"date\": link_soup.select(\"time.entry-date.updated.td-module-date\")[0].text,\n",
    "                         \"title\": link_soup.select(\"h1.entry-title\")[0].text,\n",
    "                         \"text\": link_soup.select(\"div.td-post-content\")[0].text,\n",
    "                         \"company\": platform})\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        counter += 1\n",
    "        if (counter) % 100 == 0:\n",
    "            print(\"No. links scraped: \" + str(counter))\n",
    "\n",
    "    raw = pd.DataFrame.from_dict(data)\n",
    "    raw.to_csv('data/raw/' + platform + '_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft\n",
    "parse_cpi(\"Microsoft\", 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 2287\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n",
      "No. links scraped: 1200\n",
      "No. links scraped: 1300\n",
      "No. links scraped: 1400\n",
      "No. links scraped: 1500\n",
      "No. links scraped: 1600\n",
      "No. links scraped: 1700\n",
      "No. links scraped: 1800\n",
      "No. links scraped: 1900\n",
      "No. links scraped: 2000\n",
      "No. links scraped: 2100\n",
      "No. links scraped: 2200\n"
     ]
    }
   ],
   "source": [
    "# Apple \n",
    "parse_cpi(\"Apple\", 229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "parse_cpi(\"Facebook\", 206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "parse_cpi(\"Amazon\", 188)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of links: 1145\n",
      "No. links scraped: 100\n",
      "No. links scraped: 200\n",
      "No. links scraped: 300\n",
      "No. links scraped: 400\n",
      "No. links scraped: 500\n",
      "No. links scraped: 600\n",
      "No. links scraped: 700\n",
      "No. links scraped: 800\n",
      "No. links scraped: 900\n",
      "No. links scraped: 1000\n",
      "No. links scraped: 1100\n"
     ]
    }
   ],
   "source": [
    "# Google\n",
    "parse_cpi(\"Google\", 415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge raw data for all platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.path.join('data/raw/', \"*.csv\")\n",
    "files = glob.glob(files)\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "df = df.drop(df.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date                                              title  \\\n",
      "0  March 25, 2021  Facebook’s Zuckerberg Proposes Changes To Sect...   \n",
      "1  March 24, 2021  Antitrust in a Digital World: Does It Work? – ...   \n",
      "2  March 22, 2021  US House Antitrust Chairman Has New Big Tech B...   \n",
      "3  March 22, 2021  Biden Nominates Antitrust Expert Lina Khan For...   \n",
      "4  March 19, 2021        Facebook May Face New Antitrust Probe In UK   \n",
      "\n",
      "                                                text company  \n",
      "0  This week, Facebook CEO Mark Zuckerberg propos...  Google  \n",
      "1  Below, we have provided the full transcript of...  Google  \n",
      "2  Democratic Representative David Cicilline, cha...  Google  \n",
      "3  US President Joe Biden intends to nominate Lin...  Google  \n",
      "4  Britain’s competition regulator is set to begi...  Google  \n"
     ]
    }
   ],
   "source": [
    "# remove white spaces\n",
    "df['text'] = [x.replace(\"\\n\", \"\") for x in df['text']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER to extract countries from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/tad-project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pycountry\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = spacy.load(\"en_core_web_sm\") # smaller pipeline\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = pd.read_csv(\"data/others/country_nationality_list.csv\")\n",
    "legend = legend.set_index('nationality').to_dict()['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract info from string         \n",
    "def extract(ner_function, pipeline, texts):\n",
    "    list = []\n",
    "    for text in texts:\n",
    "        raw = str(ner_function(pipeline, text))\n",
    "        list.append(str(raw))            \n",
    "    return (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner get countries and nationalities\n",
    "def get_countries(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'GPE': # country, city, state\n",
    "            if entity.text in legend.values(): # keep if in country list\n",
    "                return lm.lemmatize(entity.text)\n",
    "            else: \n",
    "                return None            \n",
    "        elif entity.label_ == 'NORP': # nationality \n",
    "            if entity.text in legend.keys(): # match to country\n",
    "                return legend[entity.text]\n",
    "            else:\n",
    "                 return None              \n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check title & text for values\n",
    "def check_both(list1, list2):\n",
    "    list = []\n",
    "    for i, j in zip(list1, list2):\n",
    "        if j != 'None':\n",
    "            list.append(j)\n",
    "        else:\n",
    "            list.append(i)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "texts = df['text']\n",
    "\n",
    "c_title = extract(get_countries, nlp_lg, titles)\n",
    "c_text = extract(get_countries, nlp_lg, texts)\n",
    "\n",
    "countries = check_both(c_title, c_text)\n",
    "\n",
    "# titles that mention cities instead of countries not captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.count('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER to extract fines from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner get fines\n",
    "def get_fines(pipeline, texts):\n",
    "    doc = pipeline(texts)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'MONEY': \n",
    "            return entity.text\n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_title = extract(get_fines, nlp_lg, titles)\n",
    "f_text = extract(get_fines, nlp_lg, texts)\n",
    "\n",
    "fines = check_both(f_title, f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11273"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fines.count('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['$1.1 billion', '$1.7 billion', '$1.8 billion', '$100 million',\n",
       "       '$10B US', '$113M Settlement', '$14.7B Deal', '$16.48 million',\n",
       "       '$17 million', '$19 billion', '$1B', '$2.1B', '$2.6B',\n",
       "       '$209B Ad-Tech', '$25 billion', '$3.4B Deal', '$300 billion',\n",
       "       '$300M Patent Verdict', '$324.5 million', '$37 billion',\n",
       "       '$392M To Settle Privacy Suit With', '$4.5 billion',\n",
       "       '$415 million', '$49 billion', '$5.1 billion',\n",
       "       '$5B Antitrust Fine Is Flawed', '$5B Fine', '$600 million',\n",
       "       '$69 Billion', '$7.4 billion', '$90M Privacy Settlement',\n",
       "       '1.49-billion-euro', '1.75B', '1.9B', '15.7bn',\n",
       "       '150 million euros', '2.3B', '24B', '30B', '39 million euros',\n",
       "       '5.7B', '660,000', '7', '7.2B', 'A$1 billion',\n",
       "       'Billions of Dollars', 'Multimillion-euro', 'None',\n",
       "       'The estimated $3.84 billion', 'US$1.36 billion', 'US$16 billion',\n",
       "       'US$26 billion', 'US$3 billion', 'US$375 million', 'US$4 billion',\n",
       "       'US$4.7 billion', 'US$5 billion', 'US$50 million',\n",
       "       'US$774 million', 'about $4.5 billion', 'around €3 billion',\n",
       "       'close to US$15 billion', 'hundreds of millions of dollars',\n",
       "       'more than $1 billion', 'more than $1 million', 'two cents',\n",
       "       '£4-trillion', '€1.49 billion', '€100 millones', '€3 billion',\n",
       "       '€500 million'], dtype='<U33')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings from distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(text, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimension of embeddings & cluster similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(text, columns = ['text'])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.text.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/tad-project/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Size\n",
       "0     -1   427\n",
       "2      1   238\n",
       "4      3   101\n",
       "3      2    61\n",
       "6      5    48\n",
       "7      6    45\n",
       "5      4    40\n",
       "8      7    21\n",
       "1      0    19"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .text\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"text\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('activision', 0.006905657198766965),\n",
       " ('deal', 0.006687424129165262),\n",
       " ('acquisition', 0.006252071693264714),\n",
       " ('cloud', 0.005573176124872544),\n",
       " ('billion', 0.005512475628019427),\n",
       " ('game', 0.005421730655356037),\n",
       " ('sony', 0.005367325511335517),\n",
       " ('announced', 0.005360809389522881),\n",
       " ('blizzard', 0.0049641807917932814),\n",
       " ('gaming', 0.004566935387692416)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ftc', 0.011437544970115957),\n",
       " ('india', 0.00926705088817797),\n",
       " ('future', 0.008121835373573348),\n",
       " ('whatsapp', 0.00698339887267274),\n",
       " ('retail', 0.006975661037723728),\n",
       " ('arbitration', 0.006559208660652846),\n",
       " ('judge', 0.006319017010858317),\n",
       " ('giphy', 0.006262419138645957),\n",
       " ('sellers', 0.006108051485879766),\n",
       " ('instagram', 0.006012050508269187)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fine', 0.01569219398302305),\n",
       " ('russian', 0.01384387122347253),\n",
       " ('russia', 0.013556155332437116),\n",
       " ('fined', 0.013427234277430276),\n",
       " ('million', 0.012642551584673468),\n",
       " ('roubles', 0.01257520786215803),\n",
       " ('moscow', 0.011553007770666547),\n",
       " ('fines', 0.0113440511570926),\n",
       " ('cnil', 0.009983435399267129),\n",
       " ('cookies', 0.009569651411018636)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[2][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tad-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cab4409ce5d224ee9075dc374e4bd34bc4b9571e804fbf24e5d7f1c9b19a09d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
